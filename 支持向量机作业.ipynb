{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 问答题\n",
    "1. 支持向量机的基本思想是什么？\n",
    "\n",
    "2. 什么是支持向量？\n",
    "\n",
    "3. 在使用 SVM 时，缩放输入值为什么很重要？\n",
    "\n",
    "4. SVM 分类器在对实例进行分类时能输出置信度分数吗？概率呢？\n",
    "\n",
    "5. 你如何在 LinearSVC、SVC 和 SGDClassifier 之间进行选择？\n",
    "\n",
    "6. 假设你已经使用 RBF 核训练了一个 SVM 分类器，但它似乎欠拟合训练集。\n",
    "   你应该增大还是减小 γ（gamma）？C 呢？\n",
    "\n",
    "7. ε 不敏感模型是什么意思？\n",
    "\n",
    "8. 使用核技巧有什么意义？"
   ],
   "id": "a0b8b340a137572d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 编程题\n",
    "1. 在葡萄酒数据集上训练SVM分类器，可以使用sklearn.datasets.load_wine()加载它。该数据集包含3个不同种植者生产的178个葡萄酒样本的化学分析：目标是训练一个分类模型，该模型能够根据葡萄酒的化学分析预测种植者。由于SVM分类器是二元分类器，将需要使用“一对全部”对所有三个类进行分类。能达到的精度是多少？\n",
    "\n",
    "   \"一对全部\"可以复习 **8_sklearn做分类.ipynb**里的笔记，里面提到了用二元分类器做多分类问题\n",
    "\n",
    "---\n",
    "\n",
    "2. 提前预习 **10_支持向量机.ipynb** 最新更新的笔记 （把SVM分类用梯度下降实现）； 大概理解笔记后，尝试自己对照笔记 实现用梯度下降实现SVM分类\n",
    "\n",
    "   并把自定义的SVM分类用于 iris data(鸢尾花数据)； 取花瓣长度 和 花瓣宽度特征， 分类 看是不是 分类2的花 （(iris.target == 2)\n",
    "\n",
    "   对比下sklearn自带的SVM分类 和 自定义SVM分类 实现的分类效果\n",
    "\n"
   ],
   "id": "f5a1dfdea31badc1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1311e0a32f236ac1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 问答题:",
   "id": "c70a96a192454ab3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:03:25.800517Z",
     "start_time": "2025-08-10T10:03:25.781929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 支持向量机的基本思想是什么？\n",
    "# SVM的核心思想是寻找一个最优超平面（决策边界）来分隔不同类别的数据，同时最大化该超平面到最近数据点（支持向量）的距离（即间隔最大化）。\n",
    "# 关键点：\n",
    "# 目标是找到一个具有最大几何间隔的分离超平面。\n",
    "# 对线性不可分数据，通过核技巧（Kernel Trick） 映射到高维空间使其线性可分。\n",
    "# 通过正则化参数（C） 控制分类错误与间隔宽度的权衡。\n",
    "\n",
    "# 2. 什么是支持向量？\n",
    "# 支持向量是距离决策边界最近的那些数据点，它们直接影响超平面的位置和方向。\n",
    "# 特点：\n",
    "# 位于间隔边界上\n",
    "# 若删除所有非支持向量，模型结果不变；但删除支持向量会改变超平面。\n",
    "# 数量通常较少，决定了模型的复杂度。\n",
    "\n",
    "# 3. 在使用 SVM 时，缩放输入值为什么很重要？\n",
    "# SVM 对特征尺度敏感，原因包括：\n",
    "# 1) 目标函数依赖特征内积:SVM 优化依赖于特征向量的点积（如核函数计算），尺度差异大的特征会使模型偏向大尺度特征。\n",
    "# 2) 间隔的定义基于距离:未缩放时，大范围特征会主导间隔计算，导致模型忽略小范围特征。\n",
    "# 3) 正则化惩罚的公平性:参数C对所有特征权重施加相同惩罚，尺度不一会导致权重更新不平衡。\n",
    "\n",
    "# 4. SVM 分类器在对实例进行分类时能输出置信度分数吗？概率呢？\n",
    "# 置信度分数:可通过 decision_function() 获取样本到超平面的符号距离（值越大表示分类置信度越高）。\n",
    "# 概率估计:使用 predict_proba()（需设置 probability=True）可通过 Platt 缩放（逻辑回归校准）输出概率，但计算开销较大且概率未必精确。"
   ],
   "id": "4210292c3c2ffc48",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5.你如何在 LinearSVC、SVC 和 SGDClassifier 之间进行选择？\n",
    "\n",
    "| 分类器           | \t适用场景\t                          | 注意事项                |\n",
    "|---------------|:--------------------------------|:--------------------|\n",
    "| LinearSVC\t    | 线性核、大规模数据（更快），支持稀疏矩阵。\t          | 需标准化数据，默认使用 L2 正则化。 |\n",
    "| SVC\t          | 中小规模数据、非线性核（RBF/多项式等），支持概率输出。\t  | 训练复杂度较高             |\n",
    "| SGDClassifier | \t超大规模数据（增量学习），                  | 支持线性SVM或自定义损失函数。\t   | 需调参（学习率、迭代次数），结果可能波动。  |\n"
   ],
   "id": "d75360418cdbcbd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:03:25.839405Z",
     "start_time": "2025-08-10T10:03:25.833963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6. 假设你已经使用 RBF 核训练了一个 SVM 分类器，但它似乎欠拟合训练集。你应该增大还是减小 γ（gamma）？C 呢？\n",
    "#增大γ（减小高斯核的带宽），γ 增大使单个样本影响范围变小，决策边界更复杂，解决欠拟合。\n",
    "# 增大C,C增大降低对误分类的容忍度，使模型更拟合训练数据。\n",
    "\n",
    "# 7. ε 不敏感模型是什么意思？\n",
    "# 定义一个以真实值为中心、宽度为2ϵ 的“容忍带”（Insensitive Zone）。若预测值落在该带内（即误差≤ϵ），则不计算损失；否则仅对超出ϵ 的误差进行惩罚。\n",
    "# 目的是解决回归问题（SVR），在预测连续值时存在的微小误差。\n",
    "#  ε 不敏感模型的支持向量:位于容忍带边界或带外的样本点\n",
    "# 关键参数：\n",
    "# ϵ（控制容忍度）\n",
    "# C（控制正则化强度）。\n",
    "# 优势：抗噪声、解稀疏、避免过拟合，特别适合含噪声的回归任务。\n",
    "\n",
    "# 8. 使用核技巧有什么意义？\n",
    "# 核技巧（Kernel Trick）允许在原始低维空间中隐式计算高维特征空间的内积，从而避免显式映射的高计算成本。\n",
    "# 核心价值：\n",
    "# 高效解决非线性问题（如 RBF 核映射到无限维）。\n",
    "# 计算复杂度仅依赖样本数，而非特征维数（避免“维数灾难”）。\n",
    "# 常见核函数：\n",
    "# 线性核(linear)、多项式核(poly)、高斯核（RBF）、Sigmoid 核。"
   ],
   "id": "f32bb369a47e6abc",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 编程题:",
   "id": "86a77570adaf6d52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:03:25.883760Z",
     "start_time": "2025-08-10T10:03:25.877156Z"
    }
   },
   "cell_type": "code",
   "source": "# 1. 在葡萄酒数据集上训练SVM分类器，可以使用sklearn.datasets.load_wine()加载它。该数据集包含3个不同种植者生产的178个葡萄酒样本的化学分析：目标是训练一个分类模型，该模型能够根据葡萄酒的化学分析预测种植者。由于SVM分类器是二元分类器，将需要使用“一对全部”对所有三个类进行分类。能达到的精度是多少？\"一对全部\"可以复习 **8_sklearn做分类.ipynb**里的笔记，里面提到了用二元分类器做多分类问题",
   "id": "27ef6716230aa83d",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:08:12.272826Z",
     "start_time": "2025-08-10T10:08:11.520761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. 加载数据集\n",
    "wine = load_wine()\n",
    "X = wine.data  # 特征数据 (178个样本 x 13个特征)\n",
    "y = wine.target  # 目标标签 (3个类别)\n",
    "print(f\"特征名: {wine.feature_names}\")\n",
    "print(f\"类别名: {wine.target_names}\")\n",
    "print(f\"数据形状: {X.shape}, 标签形状: {y.shape}\")\n",
    "\n",
    "# 2. 数据预处理（标准化）\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. 划分数据集 (70%训练, 30%测试)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 4. 创建并训练SVM分类器\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)  # 使用RBF核\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# 5. 评估模型\n",
    "y_pred = svm.predict(X_test)\n",
    "print(\"\\n模型评估报告:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"测试集准确率: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# 6. 超参数调优\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 定义参数网格\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1],\n",
    "    'kernel': ['rbf', 'linear', 'poly']\n",
    "}\n",
    "\n",
    "# 网格搜索 (耗时操作，小数据集可用)\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n最佳参数组合:\", grid_search.best_params_)\n",
    "print(\"调优后准确率:\", grid_search.best_score_)\n",
    "\n",
    "# 使用最佳模型预测\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred_best = best_svm.predict(X_test)\n",
    "print(f\"调优后测试集准确率: {accuracy_score(y_test, y_pred_best):.4f}\")"
   ],
   "id": "ce6c93959f7867a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征名: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "类别名: ['class_0' 'class_1' 'class_2']\n",
      "数据形状: (178, 13), 标签形状: (178,)\n",
      "\n",
      "模型评估报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       0.95      1.00      0.98        21\n",
      "           2       1.00      0.93      0.96        14\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n",
      "测试集准确率: 0.9815\n",
      "\n",
      "最佳参数组合: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "调优后准确率: 0.9756666666666666\n",
      "调优后测试集准确率: 0.9815\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:03:30.079017Z",
     "start_time": "2025-08-10T10:03:30.075493Z"
    }
   },
   "cell_type": "code",
   "source": "# 2. 提前预习\"10_支持向量机.ipynb\"最新更新的笔记（把SVM分类用梯度下降实现）；大概理解笔记后，尝试自己对照笔记实现用梯度下降实现SVM分类并把自定义的SVM分类用于 iris data(鸢尾花数据)；取花瓣长度和花瓣宽度特征，分类看是不是分类2的花（(iris.target == 2)对比下sklearn自带的SVM分类和自定义SVM分类实现的分类效果。",
   "id": "819b1cfe513de2b4",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:15:11.570540Z",
     "start_time": "2025-08-10T10:15:11.559811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class MyLinearSVC(BaseEstimator):\n",
    "    def __init__(self, C, eta0, n_epochs=1000, random_state=None):\n",
    "        self.C = C                   # 正则化参数\n",
    "        self._alpha = 1 / (2*C)      # 正则化系数的倒数\n",
    "        self.eta0 = eta0             # 初始学习率\n",
    "        self.n_epochs = n_epochs     # 训练轮数\n",
    "        self.random_state = random_state  # 随机种子\n",
    "\n",
    "    @property\n",
    "    def eta(self):\n",
    "        return self.eta0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 设置随机种子\n",
    "        if self.random_state:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        # 初始化权重和偏置\n",
    "        w = np.random.randn(X.shape[1], 1)  # 特征权重\n",
    "        b = 0                               # 偏置项\n",
    "\n",
    "        # 转换标签为{-1, 1}\n",
    "        t = np.array(y, dtype=np.float64).reshape(-1, 1) * 2 - 1\n",
    "\n",
    "        self.Js = []  # 存储每轮损失值\n",
    "        m = X.shape[0]  # 样本数量\n",
    "\n",
    "        # 梯度下降训练\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # 识别支持向量（违反间隔的样本）\n",
    "            support_vectors_idx = ((X@w + b)*t < 1).ravel()\n",
    "            X_sv = X[support_vectors_idx]  # 支持向量特征\n",
    "            t_sv = t[support_vectors_idx]  # 支持向量标签\n",
    "\n",
    "            # 计算损失函数（Hinge Loss + L2正则化）\n",
    "            J = (np.sum(w * w) * self._alpha + np.sum(1 - t_sv * (X_sv@w + b))) / m\n",
    "            self.Js.append(J)\n",
    "\n",
    "            # 计算梯度\n",
    "            w_gradient_vector = (2*self._alpha*w - X_sv.T @ t_sv) / m\n",
    "            b_derivative = -np.sum(t_sv) / m\n",
    "\n",
    "            # 更新参数\n",
    "            w = w - self.eta * w_gradient_vector\n",
    "            b = b - self.eta * b_derivative\n",
    "\n",
    "        # 保存训练结果\n",
    "        self.intercept_ = b\n",
    "        self.coef_ = w\n",
    "        # 识别最终支持向量\n",
    "        support_vectors_idx = ((X@w + b)*t < 1).ravel()\n",
    "        self.support_vectors_ = X[support_vectors_idx]\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return X.dot(self.coef_) + self.intercept_\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.decision_function(X) >= 0\n"
   ],
   "id": "eedd3c4e6527bf2b",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:15:14.566387Z",
     "start_time": "2025-08-10T10:15:14.533611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris=load_iris()\n",
    "X=iris.data\n",
    "y=iris.target==2\n",
    "print(f\"特征名: {iris.feature_names}\")\n",
    "print(f\"类别名: {iris.target_names}\")\n",
    "print(f\"数据形状: {X.shape}, 标签形状: {y.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "C = 1\n",
    "my_linearSVC = make_pipeline(StandardScaler(), MyLinearSVC(C=C, eta0 =0.1, n_epochs=500, random_state=42))\n",
    "my_linearSVC.fit(X, y)\n",
    "\n",
    "y_pred = my_linearSVC.predict(X_test)\n",
    "print(\"\\n模型评估报告:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"测试集准确率: {accuracy_score(y_test, y_pred):.4f}\")"
   ],
   "id": "e36ebbf523155852",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征名: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "类别名: ['setosa' 'versicolor' 'virginica']\n",
      "数据形状: (150, 4), 标签形状: (150,)\n",
      "\n",
      "模型评估报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00        32\n",
      "        True       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "测试集准确率: 1.0000\n"
     ]
    }
   ],
   "execution_count": 45
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
